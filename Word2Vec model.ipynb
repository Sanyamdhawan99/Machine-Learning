{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = list(stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file):\n",
    "    f = open(file, 'r')\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    #print(sentences)\n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        # convert every word to lower case and remove stop words\n",
    "        words = [w.lower() for w in words if len(w) > 2 and w not in stopwords]\n",
    "        data.append(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deepika',\n",
       "  'padukone',\n",
       "  'ranveer',\n",
       "  'singh',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding',\n",
       "  'intrigued',\n",
       "  'everyone'],\n",
       " [\"n't\"],\n",
       " ['while',\n",
       "  'busy',\n",
       "  'tracking',\n",
       "  'every',\n",
       "  'aspect',\n",
       "  'sonam',\n",
       "  'kapoor',\n",
       "  'anand',\n",
       "  'ahuja',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding',\n",
       "  'two',\n",
       "  'tigers',\n",
       "  'delhi',\n",
       "  'zoo',\n",
       "  'secretly',\n",
       "  'tied',\n",
       "  'knot'],\n",
       " ['the',\n",
       "  'zoo',\n",
       "  'confirmed',\n",
       "  'today',\n",
       "  'big',\n",
       "  'cats',\n",
       "  'married',\n",
       "  'novel',\n",
       "  'attempt',\n",
       "  'get',\n",
       "  'yellow-striped',\n",
       "  'royal',\n",
       "  'bengal',\n",
       "  'mate',\n",
       "  'white',\n",
       "  'tiger',\n",
       "  'produce',\n",
       "  'healthier',\n",
       "  'mixed',\n",
       "  'offspring'],\n",
       " ['from',\n",
       "  'pastel',\n",
       "  'designer',\n",
       "  'ensembles',\n",
       "  'tuscan',\n",
       "  'venue',\n",
       "  'woman',\n",
       "  'made',\n",
       "  'virushka',\n",
       "  'wedding',\n",
       "  'success'],\n",
       " ['abhishek',\n",
       "  'son',\n",
       "  'actors',\n",
       "  'amitabh',\n",
       "  'jaya',\n",
       "  'bachchan',\n",
       "  'made',\n",
       "  'debut',\n",
       "  '2000',\n",
       "  'refugee',\n",
       "  'co-starring',\n",
       "  'kareena',\n",
       "  'kapoor'],\n",
       " ['abhishek',\n",
       "  'also',\n",
       "  'featured',\n",
       "  'films',\n",
       "  'like',\n",
       "  'dhoom',\n",
       "  'bunty',\n",
       "  'aur',\n",
       "  'babli',\n",
       "  'kabhi',\n",
       "  'alvida',\n",
       "  'naa',\n",
       "  'kehna',\n",
       "  'bluffmaster'],\n",
       " ['paa', 'last', 'seen', '2015', 'all', 'well'],\n",
       " ['bring',\n",
       "  'inside',\n",
       "  'scoop',\n",
       "  'menu',\n",
       "  'shahid',\n",
       "  'kapoor',\n",
       "  'mira',\n",
       "  'rajput',\n",
       "  'wedding',\n",
       "  'celebrations'],\n",
       " ['wondering', 'shahid', 'mira', 'savoured', 'loved', 'functions'],\n",
       " ['click', 'find'],\n",
       " ['pataudis',\n",
       "  'gear',\n",
       "  'soha',\n",
       "  'ali',\n",
       "  'khan',\n",
       "  'kunal',\n",
       "  'khemu',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding'],\n",
       " ['arpita',\n",
       "  'aayush',\n",
       "  'married',\n",
       "  'hyderabad',\n",
       "  'falaknuma',\n",
       "  'palace',\n",
       "  'november',\n",
       "  'according',\n",
       "  'punjabi',\n",
       "  'hindu',\n",
       "  'rituals'],\n",
       " ['arpita',\n",
       "  'khan',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding',\n",
       "  'begun',\n",
       "  'hyderabad',\n",
       "  'falaknuma',\n",
       "  'palace',\n",
       "  'look',\n",
       "  'wearing'],\n",
       " ['guest-list',\n",
       "  'ahaana',\n",
       "  'deol',\n",
       "  'delhi',\n",
       "  'businessman',\n",
       "  'vaibhav',\n",
       "  'vohra',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding',\n",
       "  'wedding',\n",
       "  'reception',\n",
       "  'delhi',\n",
       "  'attended',\n",
       "  'bjp',\n",
       "  'vips',\n",
       "  'awe-struck',\n",
       "  'videos',\n",
       "  'sangeet',\n",
       "  'wedding',\n",
       "  'make',\n",
       "  'fall',\n",
       "  'love',\n",
       "  'love'],\n",
       " ['ahana',\n",
       "  'deol',\n",
       "  'second',\n",
       "  'daughter',\n",
       "  'actors',\n",
       "  'hema',\n",
       "  'malini',\n",
       "  'dharmendra',\n",
       "  'married',\n",
       "  'businessman',\n",
       "  'vaibhav',\n",
       "  'vora',\n",
       "  'mumbai',\n",
       "  'big',\n",
       "  'fat',\n",
       "  'bollywood',\n",
       "  'wedding',\n",
       "  'february'],\n",
       " ['priyanka',\n",
       "  'nick',\n",
       "  'jonas',\n",
       "  'wedding',\n",
       "  'attracted',\n",
       "  'millions',\n",
       "  'eye',\n",
       "  'balls',\n",
       "  'high',\n",
       "  'end',\n",
       "  'wedding',\n",
       "  'cheerfully',\n",
       "  'done',\n",
       "  'udaipur',\n",
       "  'rajasthan',\n",
       "  '1st',\n",
       "  'december',\n",
       "  '2018'],\n",
       " ['after',\n",
       "  'pictures',\n",
       "  'got',\n",
       "  'released',\n",
       "  'wedding',\n",
       "  'functions',\n",
       "  'found',\n",
       "  'happy',\n",
       "  'families'],\n",
       " ['christain',\n",
       "  'rituals',\n",
       "  'marriage',\n",
       "  'ceremonies',\n",
       "  'completed',\n",
       "  'kissed',\n",
       "  'said'],\n",
       " ['10th',\n",
       "  'may',\n",
       "  '2018',\n",
       "  'pleasant',\n",
       "  'news',\n",
       "  'come',\n",
       "  'around',\n",
       "  'neha',\n",
       "  'weds',\n",
       "  'angad',\n",
       "  'bedi',\n",
       "  'got',\n",
       "  'married',\n",
       "  'gurudwara'],\n",
       " ['neha',\n",
       "  'informed',\n",
       "  'tweet',\n",
       "  'best',\n",
       "  'decision',\n",
       "  'life..',\n",
       "  'today',\n",
       "  'married',\n",
       "  'best',\n",
       "  'friend'],\n",
       " ['hello', 'husband'],\n",
       " ['would', 'like', 'congratulate'],\n",
       " ['recently',\n",
       "  'wedding',\n",
       "  'date',\n",
       "  'got',\n",
       "  'announced',\n",
       "  'sonam',\n",
       "  'kapoor',\n",
       "  'wedding',\n",
       "  'finally',\n",
       "  'today',\n",
       "  'day',\n",
       "  'arrived',\n",
       "  'actress',\n",
       "  'sonam',\n",
       "  'kapoor',\n",
       "  'hitched',\n",
       "  'anand',\n",
       "  'ahuja',\n",
       "  'businessman',\n",
       "  'belong',\n",
       "  'reputed',\n",
       "  'business',\n",
       "  'family'],\n",
       " ['sonam',\n",
       "  'wedding',\n",
       "  'reception',\n",
       "  'happened',\n",
       "  'the',\n",
       "  'leela',\n",
       "  'whereas',\n",
       "  'wedding',\n",
       "  'affair',\n",
       "  'afternoon',\n",
       "  'day',\n",
       "  '8th',\n",
       "  'may',\n",
       "  '2018',\n",
       "  'happened',\n",
       "  'relative',\n",
       "  'bunglow'],\n",
       " ['big',\n",
       "  'boss',\n",
       "  'fame',\n",
       "  'prince',\n",
       "  'narula',\n",
       "  'yuvika',\n",
       "  'chaudhary',\n",
       "  'recently',\n",
       "  'got',\n",
       "  'engaged',\n",
       "  'january',\n",
       "  '2018',\n",
       "  'shared',\n",
       "  'news',\n",
       "  'fans',\n",
       "  'social',\n",
       "  'media'],\n",
       " ['they', 'bonded', 'well', 'big', 'boss', 'season'],\n",
       " ['they',\n",
       "  'often',\n",
       "  'used',\n",
       "  'share',\n",
       "  'pictures',\n",
       "  'social',\n",
       "  'media',\n",
       "  'handles',\n",
       "  'prince',\n",
       "  'renamed',\n",
       "  'social',\n",
       "  'media',\n",
       "  'account',\n",
       "  'prince',\n",
       "  'yuvika',\n",
       "  'narula'],\n",
       " ['this',\n",
       "  'lovebirds',\n",
       "  'tie',\n",
       "  'knot',\n",
       "  'year',\n",
       "  'end',\n",
       "  '2018.the',\n",
       "  'biggest',\n",
       "  'wedding',\n",
       "  'year',\n",
       "  'top-rated',\n",
       "  'list',\n",
       "  'bollywood',\n",
       "  'marriages',\n",
       "  '2017',\n",
       "  'secret',\n",
       "  'marriages',\n",
       "  'bollywood',\n",
       "  'celebrity',\n",
       "  'anushka',\n",
       "  'sharma',\n",
       "  'got',\n",
       "  'hitched',\n",
       "  'caption',\n",
       "  'indian',\n",
       "  'cricket',\n",
       "  'team',\n",
       "  'virat',\n",
       "  'kohli',\n",
       "  '11th',\n",
       "  'december',\n",
       "  'dating',\n",
       "  'years'],\n",
       " ['who',\n",
       "  'shooting',\n",
       "  'advertisement',\n",
       "  'anti-dandruff',\n",
       "  'shampoo',\n",
       "  'get',\n",
       "  'together',\n",
       "  'life'],\n",
       " ['the',\n",
       "  'wedding',\n",
       "  'hush-hush',\n",
       "  'affair',\n",
       "  'took',\n",
       "  'place',\n",
       "  'tuscany',\n",
       "  'italy',\n",
       "  'close',\n",
       "  'friends',\n",
       "  'family'],\n",
       " ['former',\n",
       "  'indian',\n",
       "  'fast',\n",
       "  'bowler',\n",
       "  'zaheer',\n",
       "  'khan',\n",
       "  'got',\n",
       "  'married',\n",
       "  'chak',\n",
       "  'india',\n",
       "  'fame',\n",
       "  'actress',\n",
       "  'sagarika',\n",
       "  'ghatge',\n",
       "  '23rd',\n",
       "  'november'],\n",
       " ['they',\n",
       "  'dating',\n",
       "  'months',\n",
       "  'preferred',\n",
       "  'low',\n",
       "  'key',\n",
       "  'wedding',\n",
       "  'rather',\n",
       "  'lavish',\n",
       "  'one'],\n",
       " ['but',\n",
       "  'grand',\n",
       "  'reception',\n",
       "  'organized',\n",
       "  'friends',\n",
       "  'family.here',\n",
       "  'another',\n",
       "  'grand',\n",
       "  'wedding',\n",
       "  '2017'],\n",
       " ['laughter',\n",
       "  'queen',\n",
       "  'bharti',\n",
       "  'tied',\n",
       "  'knot',\n",
       "  'longtime',\n",
       "  'beau',\n",
       "  'haarh',\n",
       "  '3rd',\n",
       "  'december'],\n",
       " ['they', 'theme', 'wedding', 'held', 'beach', 'city', 'goa']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = readFile(\"bollywood.txt\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import keyedvectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=305, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(text, size=300, window=100, min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepika', 'padukone', 'ranveer', 'singh', 'big', 'fat', 'bollywood', 'wedding', 'intrigued', 'everyone', \"n't\", 'while', 'busy', 'tracking', 'every', 'aspect', 'sonam', 'kapoor', 'anand', 'ahuja', 'two', 'tigers', 'delhi', 'zoo', 'secretly', 'tied', 'knot', 'the', 'confirmed', 'today', 'cats', 'married', 'novel', 'attempt', 'get', 'yellow-striped', 'royal', 'bengal', 'mate', 'white', 'tiger', 'produce', 'healthier', 'mixed', 'offspring', 'from', 'pastel', 'designer', 'ensembles', 'tuscan', 'venue', 'woman', 'made', 'virushka', 'success', 'abhishek', 'son', 'actors', 'amitabh', 'jaya', 'bachchan', 'debut', '2000', 'refugee', 'co-starring', 'kareena', 'also', 'featured', 'films', 'like', 'dhoom', 'bunty', 'aur', 'babli', 'kabhi', 'alvida', 'naa', 'kehna', 'bluffmaster', 'paa', 'last', 'seen', '2015', 'all', 'well', 'bring', 'inside', 'scoop', 'menu', 'shahid', 'mira', 'rajput', 'celebrations', 'wondering', 'savoured', 'loved', 'functions', 'click', 'find', 'pataudis', 'gear', 'soha', 'ali', 'khan', 'kunal', 'khemu', 'arpita', 'aayush', 'hyderabad', 'falaknuma', 'palace', 'november', 'according', 'punjabi', 'hindu', 'rituals', 'begun', 'look', 'wearing', 'guest-list', 'ahaana', 'deol', 'businessman', 'vaibhav', 'vohra', 'reception', 'attended', 'bjp', 'vips', 'awe-struck', 'videos', 'sangeet', 'make', 'fall', 'love', 'ahana', 'second', 'daughter', 'hema', 'malini', 'dharmendra', 'vora', 'mumbai', 'february', 'priyanka', 'nick', 'jonas', 'attracted', 'millions', 'eye', 'balls', 'high', 'end', 'cheerfully', 'done', 'udaipur', 'rajasthan', '1st', 'december', '2018', 'after', 'pictures', 'got', 'released', 'found', 'happy', 'families', 'christain', 'marriage', 'ceremonies', 'completed', 'kissed', 'said', '10th', 'may', 'pleasant', 'news', 'come', 'around', 'neha', 'weds', 'angad', 'bedi', 'gurudwara', 'informed', 'tweet', 'best', 'decision', 'life..', 'friend', 'hello', 'husband', 'would', 'congratulate', 'recently', 'date', 'announced', 'finally', 'day', 'arrived', 'actress', 'hitched', 'belong', 'reputed', 'business', 'family', 'happened', 'leela', 'whereas', 'affair', 'afternoon', '8th', 'relative', 'bunglow', 'boss', 'fame', 'prince', 'narula', 'yuvika', 'chaudhary', 'engaged', 'january', 'shared', 'fans', 'social', 'media', 'they', 'bonded', 'season', 'often', 'used', 'share', 'handles', 'renamed', 'account', 'this', 'lovebirds', 'tie', 'year', '2018.the', 'biggest', 'top-rated', 'list', 'marriages', '2017', 'secret', 'celebrity', 'anushka', 'sharma', 'caption', 'indian', 'cricket', 'team', 'virat', 'kohli', '11th', 'dating', 'years', 'who', 'shooting', 'advertisement', 'anti-dandruff', 'shampoo', 'together', 'life', 'hush-hush', 'took', 'place', 'tuscany', 'italy', 'close', 'friends', 'former', 'fast', 'bowler', 'zaheer', 'chak', 'india', 'sagarika', 'ghatge', '23rd', 'months', 'preferred', 'low', 'key', 'rather', 'lavish', 'one', 'but', 'grand', 'organized', 'family.here', 'another', 'laughter', 'queen', 'bharti', 'longtime', 'beau', 'haarh', '3rd', 'theme', 'held', 'beach', 'city', 'goa']\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8.57205014e-04,  1.64707599e-03,  1.29352952e-03, -7.76615401e-04,\n",
       "       -5.29590470e-04, -1.50710938e-03, -4.93662315e-04, -1.35544818e-04,\n",
       "        4.76353278e-04, -1.42150838e-03, -1.39815034e-03, -1.29073276e-03,\n",
       "        1.08638627e-03, -7.18179566e-04,  1.37093055e-04, -1.60196610e-03,\n",
       "        8.33784172e-04, -4.66072961e-04,  1.28155481e-03,  1.32869428e-03,\n",
       "        4.58246854e-04, -7.85488286e-04, -8.43082031e-04, -4.44817648e-04,\n",
       "        5.37695480e-04,  6.41435399e-05, -6.10700052e-04, -4.08478547e-04,\n",
       "       -9.80201061e-04,  2.11903985e-04,  1.49902049e-03, -1.49897195e-03,\n",
       "       -1.62691809e-03, -1.22672599e-03,  1.53561751e-03,  1.51019805e-04,\n",
       "        6.99279946e-04,  3.88095272e-04, -5.07663819e-04,  5.04471827e-04,\n",
       "        1.24964770e-03, -1.63139857e-03,  1.27741625e-03,  6.76286290e-04,\n",
       "       -3.96482355e-05, -9.96179646e-04,  1.05850631e-03, -4.73622582e-04,\n",
       "       -1.08162127e-03, -3.48014233e-04,  2.31376252e-04,  1.23794400e-03,\n",
       "       -1.10177312e-03, -7.92874780e-04,  1.49960819e-04,  1.19739806e-03,\n",
       "        3.97231139e-04, -4.75305453e-04,  8.87782371e-04,  9.03000706e-04,\n",
       "       -2.57085776e-04, -9.75991250e-04,  1.52645924e-03,  1.01503334e-03,\n",
       "       -1.10964000e-03, -1.66235550e-03, -1.60319614e-03,  9.44465981e-04,\n",
       "       -3.13822311e-05,  1.40229310e-03,  3.24052060e-04, -1.41733815e-03,\n",
       "       -1.54366333e-03, -1.34610804e-03, -1.78318762e-04, -3.04717134e-04,\n",
       "        1.02312304e-03, -3.50131886e-04, -1.36892858e-03, -5.69707190e-04,\n",
       "        3.93164155e-05,  6.42040221e-04, -1.33930799e-03, -6.95252733e-04,\n",
       "        1.56497976e-04, -1.44630016e-04,  1.55277655e-03, -7.53276108e-04,\n",
       "       -1.22611318e-03,  1.48123933e-03,  1.67593698e-03, -1.11866219e-03,\n",
       "       -1.59780972e-03,  1.52067130e-03,  1.23915437e-03, -1.05194980e-03,\n",
       "        1.05702842e-03, -3.19455830e-05, -8.39887653e-04, -1.37535855e-03,\n",
       "       -8.97011836e-04,  1.02643226e-03, -9.90574365e-04, -9.58494726e-04,\n",
       "        5.88615309e-04, -5.38508466e-04,  1.15367281e-03,  9.26860957e-04,\n",
       "       -8.28251534e-04,  5.59190230e-04, -1.26758765e-03,  6.04606583e-04,\n",
       "        1.38515222e-03, -1.26672472e-04, -1.18297598e-04, -7.11154062e-05,\n",
       "       -3.11080948e-04, -3.85620253e-04,  6.80133351e-04,  1.10090640e-03,\n",
       "        8.91234376e-04, -5.88574039e-04,  1.39102538e-03, -1.72766464e-04,\n",
       "        7.13418587e-04, -1.37139426e-03,  6.38743339e-04, -1.51186925e-03,\n",
       "       -1.38744223e-03, -8.58928470e-05,  9.56535223e-04, -1.53415732e-03,\n",
       "        6.59042445e-04, -4.72101179e-04, -3.87452164e-05,  1.58239316e-04,\n",
       "        9.45085252e-04,  1.20883342e-03,  1.45969709e-04, -1.06847764e-03,\n",
       "        2.39683548e-04,  9.48875910e-04, -1.31699513e-03,  1.45152898e-03,\n",
       "        1.54120103e-03,  3.27453628e-04,  3.97629628e-04, -9.93896741e-04,\n",
       "       -1.15508534e-04,  2.57074833e-04, -2.74533348e-04, -3.86712782e-04,\n",
       "        1.62276404e-03, -4.10811073e-04, -1.64222380e-03, -1.53719957e-05,\n",
       "       -4.09830900e-05, -6.37309102e-04, -1.55502232e-04,  7.13395246e-04,\n",
       "        1.03188469e-03,  2.18157933e-04, -1.15872885e-03, -1.54626230e-03,\n",
       "        1.07411272e-03,  4.78719157e-04, -1.29239401e-03, -3.83509760e-04,\n",
       "       -5.47852018e-04, -2.74224934e-04,  1.51850830e-03, -1.18363637e-03,\n",
       "        1.20153138e-03,  9.32934636e-04,  1.00459601e-03, -1.27254333e-03,\n",
       "       -4.39254014e-04, -2.62894726e-04, -6.77553995e-04,  1.43507868e-03,\n",
       "        6.43220381e-04, -1.29516225e-03, -4.50184860e-04,  1.53903966e-03,\n",
       "        9.52739094e-04, -6.68377383e-04, -1.30827643e-03,  1.53789169e-03,\n",
       "        1.13309349e-03, -1.57322106e-03,  7.01797762e-05, -9.92360758e-04,\n",
       "        1.03531196e-03, -4.82751726e-04,  6.56621240e-04, -1.65058579e-03,\n",
       "       -6.91932917e-04, -8.75809579e-04,  5.72413846e-04, -9.55360534e-04,\n",
       "        1.32066582e-03, -8.16789398e-04, -7.97644607e-04, -1.50959604e-04,\n",
       "       -1.00196956e-03,  1.18105533e-03, -4.26040235e-04, -1.54210208e-03,\n",
       "       -2.47559918e-04,  1.19776632e-04, -1.57831877e-03, -7.68805097e-04,\n",
       "        1.24789239e-03,  6.43708801e-04,  1.32415234e-03,  8.61692417e-04,\n",
       "       -2.64756352e-04,  1.62070291e-03, -1.51892169e-03, -1.23708730e-03,\n",
       "        1.30893034e-03, -3.99967132e-04,  1.34429440e-03, -1.20180845e-03,\n",
       "        2.21539685e-05,  4.47241982e-05,  5.83311426e-04,  1.45573006e-03,\n",
       "       -1.41891197e-03,  1.48842658e-03,  1.21572090e-03, -9.86338011e-04,\n",
       "        1.18693616e-03, -1.42134516e-03, -2.10866318e-04, -8.52152123e-04,\n",
       "        5.80062333e-04, -6.75466552e-04,  1.20407820e-03, -1.35441209e-04,\n",
       "       -1.56608876e-04, -7.80920382e-04, -6.76420241e-05,  2.61127105e-04,\n",
       "       -1.57755960e-04, -1.56554882e-03, -1.10536453e-03,  8.92931930e-05,\n",
       "       -2.64765607e-04, -3.94049886e-04,  1.15498458e-03, -1.63722131e-03,\n",
       "        9.22575593e-04, -1.29414233e-03,  1.58090808e-03,  7.29363936e-04,\n",
       "        8.17561348e-04,  1.01633072e-04,  1.61763848e-04, -7.65877194e-04,\n",
       "        5.39844623e-04, -1.57442852e-03, -5.77138329e-04,  8.45347880e-04,\n",
       "       -3.22342356e-04, -1.25059125e-03,  1.59255147e-03,  8.25110837e-05,\n",
       "        4.07396583e-04,  1.25179498e-03, -1.58875191e-03, -8.62109882e-04,\n",
       "        1.49926951e-03, -8.03459843e-04, -6.25207264e-04,  6.94766873e-04,\n",
       "       -1.45950145e-03,  6.98583666e-04,  1.49164675e-03,  5.71696844e-04,\n",
       "       -8.23585549e-04, -5.13884625e-05, -6.39710750e-04, -4.11446439e-04,\n",
       "        6.36523881e-04,  5.22128888e-04, -1.19091989e-03,  1.61145383e-03,\n",
       "        1.66037783e-03, -8.60964356e-04,  8.52958241e-04,  9.30119306e-04,\n",
       "        8.07986653e-04, -1.57755206e-03, -1.29866914e-03, -1.36526232e-03,\n",
       "       -1.23282115e-03,  6.37875055e-04,  1.19920226e-03, -1.46825390e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model[\"deepika\"].shape)\n",
    "model[\"deepika\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_actor(a, b, c, word_vectors):\n",
    "    # accepts a, b, c such tha is a->b then c->d \n",
    "    # we have to find d\n",
    "    a, b, c = a.lower(), b.lower(), c.lower()\n",
    "    max_similarity = -100\n",
    "    d = None\n",
    "    wa, wb, wc = word_vectors[a], word_vectors[b], word_vectors[c]\n",
    "    options = [\"ranveer\", \"deepika\", \"padukone\", \"singh\", \"virat\", \"kohli\", 'arpita', 'aayush', 'sagarika', \"priyanka\", \"nick\", \"jonas\", \"ahana\", \"sonam\", \"kapoor\", \"deol\", \"angad\", \"bedi\", \"anushka\", \"sharma\", \"prince\", \"yuvika\",\"chaudhary\", \"narula\", \"amitabh\", \"jaya\", \"bachchan\"]\n",
    "    for w in options:\n",
    "        if w in [a, b, c]:\n",
    "            continue\n",
    "        wd = word_vectors[w]\n",
    "        sim = cosine_similarity([wb - wa], [wd - wc])\n",
    "        print(\"sim: \", w, \" \", sim)\n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim:  ranveer   [[-0.01291711]]\n",
      "sim:  padukone   [[-0.07902163]]\n",
      "sim:  singh   [[0.0009568]]\n",
      "sim:  arpita   [[0.00249517]]\n",
      "sim:  aayush   [[-0.04148054]]\n",
      "sim:  sagarika   [[-0.02871965]]\n",
      "sim:  priyanka   [[-0.02916125]]\n",
      "sim:  nick   [[-0.00640139]]\n",
      "sim:  jonas   [[0.00566776]]\n",
      "sim:  ahana   [[-0.07360701]]\n",
      "sim:  sonam   [[-0.04841142]]\n",
      "sim:  kapoor   [[-0.0125048]]\n",
      "sim:  deol   [[-0.05525108]]\n",
      "sim:  angad   [[0.01522597]]\n",
      "sim:  bedi   [[-0.01693206]]\n",
      "sim:  anushka   [[-0.02912299]]\n",
      "sim:  sharma   [[-0.02247269]]\n",
      "sim:  prince   [[0.00795082]]\n",
      "sim:  yuvika   [[-0.01707886]]\n",
      "sim:  chaudhary   [[-0.04797444]]\n",
      "sim:  narula   [[-0.02108493]]\n",
      "sim:  amitabh   [[-0.0251934]]\n",
      "sim:  jaya   [[0.03329548]]\n",
      "sim:  bachchan   [[0.03069237]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jaya'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"virat\", \"kohli\", \"deepika\")\n",
    "predict_actor(*triad, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
